{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook allows you to view the results of all the classes implemented in the context of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 1\n",
    "\n",
    "## Exercise 1 -> Made on the exercise_1.ipynb on this folder\n",
    "\n",
    "PATH: /si/exercices_evaluation/exercise_1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2.1 -> Dropna Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample  before dropna:\n",
      "[[4.9 3.  1.4 0.2]\n",
      " [4.7 nan 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "Random sample  after dropna:\n",
      "[[4.9 3.  1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "\n",
    "df_na = read_csv('../datasets/iris/iris_missing_data.csv', sep=',', features=True, label=True)\n",
    "\n",
    "print('Random sample  before dropna:')\n",
    "print(df_na.X[1:4])\n",
    "df_na.dropna()\n",
    "print('Random sample  after dropna:')\n",
    "print(df_na.X[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 -> Fillna Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample before fillna:\n",
      "[4.7 nan 1.3 0.2]\n",
      "Random sample after fillna:\n",
      "[4.7 0.  1.3 0.2]\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.io.csv_file import read_csv\n",
    "\n",
    "df_na = read_csv('../datasets/iris/iris_missing_data.csv', sep=',', features=True, label=True)\n",
    "\n",
    "print('Random sample before fillna:')\n",
    "print(df_na.X[2])\n",
    "df_na.fillna(0)\n",
    "print('Random sample after fillna:')\n",
    "print(df_na.X[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3 -> Remove_by_index Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample before remove_by_index:\n",
      "[[4.9 3.  1.4 0.2]\n",
      " [4.7 nan 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "Random sample after remove_by_index:\n",
      "[[4.9 3.  1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n"
     ]
    }
   ],
   "source": [
    "df_na = read_csv('../datasets/iris/iris_missing_data.csv', sep=',', features=True, label=True)\n",
    "\n",
    "print('Random sample before remove_by_index:')\n",
    "print(df_na.X[1:4])\n",
    "df_na.remove_by_index(2)\n",
    "print('Random sample after remove_by_index:')\n",
    "print(df_na.X[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Evaluation\n",
    "More examples of how to use these methods\n",
    "to the script/notebook of Exercise 1 \n",
    "\n",
    "PATH: /si/scripts/dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2\n",
    "\n",
    "## Exercise 3.1/3.3 -> SelectPercentile Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 4\n",
      "Percentile = 0 , Selected Features = 4\n",
      "Percentile = 0.25 , Selected Features = 1\n",
      "Percentile = 0.5 , Selected Features = 2\n",
      "Percentile = 0.75 , Selected Features = 3\n",
      "Percentile = 1.0 , Selected Features = 4\n"
     ]
    }
   ],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.feature_selection.select_percentile import Selectpercentile\n",
    "from si.statistics.f_classification import f_classification\n",
    "\n",
    "file = \"/home/sa_bruno/Documentos/GitHub/si/datasets/iris/iris.csv\"\n",
    "dataset = read_csv(file, features= True, label=True)\n",
    "print (f\"Total = {len(dataset.features)}\")\n",
    "\n",
    "perc_0 = 0\n",
    "perc_0_25 = 0.25\n",
    "perc_0_50 = 0.50\n",
    "perc_0_75 = 0.75\n",
    "perc_1 = 1.0\n",
    "\n",
    "select_percentile = Selectpercentile(f_classification, perc_0)\n",
    "filtered_dataset =  select_percentile.fit_transform(dataset)\n",
    "print(f\"Percentile = {perc_0} , Selected Features = {len(filtered_dataset.features)}\")\n",
    "\n",
    "\n",
    "select_percentile = Selectpercentile(f_classification, perc_0_25)\n",
    "filtered_dataset =  select_percentile.fit_transform(dataset)\n",
    "print(f\"Percentile = {perc_0_25} , Selected Features = {len(filtered_dataset.features)}\")\n",
    "\n",
    "select_percentile = Selectpercentile(f_classification, perc_0_50)\n",
    "filtered_dataset =  select_percentile.fit_transform(dataset)\n",
    "print(f\"Percentile = {perc_0_50} , Selected Features = {len(filtered_dataset.features)}\")\n",
    "\n",
    "select_percentile = Selectpercentile(f_classification,perc_0_75)\n",
    "filtered_dataset =  select_percentile.fit_transform(dataset)\n",
    "print(f\"Percentile = {perc_0_75} , Selected Features = {len(filtered_dataset.features)}\")\n",
    "\n",
    "select_percentile = Selectpercentile(f_classification, perc_1)\n",
    "filtered_dataset =  select_percentile.fit_transform(dataset)\n",
    "print(f\"Percentile = {perc_1} , Selected Features = {len(filtered_dataset.features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional examples on:\n",
    "\n",
    "PATH: /si/scripts/feature_selection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3\n",
    "## Exercise 4 -> Implementation of the Manhattan distance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our distance = [0 9]\n",
      "sklearn distance = [[0. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import manhattan_distances as manhattan_distances_skl\n",
    "from si.statistics.manhattan_distance import manhattan_distance\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "our_distance = manhattan_distance(x, y)\n",
    "    \n",
    "# using sklearn    \n",
    "sklearn_distance = manhattan_distances_skl(x.reshape(1, -1), y)\n",
    "assert np.allclose(our_distance, sklearn_distance)\n",
    "\n",
    "print(f\"our distance = {our_distance}\")\n",
    "print(f\"sklearn distance = {sklearn_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.1 -> Implementation of the PCA Class Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [4.50000000e+00 1.66666667e-01 1.33372984e-33]\n",
      "\n",
      "Components: [[ 1.11022302e-16  1.96116135e-01 -9.80580676e-01  0.00000000e+00]\n",
      " [ 5.55111512e-17 -9.80580676e-01 -1.96116135e-01  0.00000000e+00]\n",
      " [-1.00000000e+00 -1.11022302e-16  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "Mean: [0.         1.33333333 1.66666667 3.        ]\n",
      "\n",
      "Original data: [[0 2 0 3]\n",
      " [0 1 4 3]\n",
      " [0 1 1 3]]\n",
      "\n",
      "Transformed data: [[ 1.76504522e+00 -3.26860225e-01 -7.40148683e-17]\n",
      " [-2.35339362e+00 -1.30744090e-01  3.70074342e-17]\n",
      " [ 5.88348405e-01  4.57604315e-01  3.70074342e-17]]\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.decomposition.pca import PCA\n",
    "\n",
    "\n",
    "dataset = Dataset(X=np.array([[0, 2, 0, 3],\n",
    "                            [0, 1, 4, 3],\n",
    "                            [0, 1, 1, 3]]),\n",
    "                y=np.array([0, 1, 0]),\n",
    "                features=[\"f1\", \"f2\", \"f3\", \"f4\"],\n",
    "                label=\"y\")\n",
    "        \n",
    "pca = PCA(n_components=3)\n",
    "pca.fit_transform(dataset.X)\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance}\")\n",
    "print()\n",
    "print(f\"Components: {pca.components}\")\n",
    "print()\n",
    "print(f\"Mean: {pca.mean}\")\n",
    "print()\n",
    "print(f\"Original data: {dataset.X}\")\n",
    "print()\n",
    "print(f\"Transformed data: {pca.fit_transform(dataset.X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 -> Test of the PCA Class using iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance\n",
      "Our\n",
      "[4.22484077 0.24224357 0.07852391]\n",
      "\n",
      "Sklearn\n",
      "[4.22484077 0.24224357 0.07852391]\n",
      "\n",
      "\n",
      "Components\n",
      "Our\n",
      "[[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [-0.65653988 -0.72971237  0.1757674   0.07470647]\n",
      " [ 0.58099728 -0.59641809 -0.07252408 -0.54906091]]\n",
      "\n",
      "Sklearn\n",
      "[[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [ 0.65653988  0.72971237 -0.1757674  -0.07470647]\n",
      " [-0.58099728  0.59641809  0.07252408  0.54906091]]\n",
      "\n",
      "\n",
      "Mean\n",
      "Our\n",
      "[5.84333333 3.054      3.75866667 1.19866667]\n",
      "\n",
      "Sklearn\n",
      "[5.84333333 3.054      3.75866667 1.19866667]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Iris dataset\n",
    "\n",
    "from sklearn.decomposition import PCA as PCA_skl\n",
    "from si.decomposition.pca import PCA\n",
    "\n",
    "data = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/iris/iris.csv', sep=',', features=True, label=True)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit_transform(data.X)\n",
    "\n",
    "pca_skl = PCA_skl(n_components=3)\n",
    "pca_skl.fit_transform(data.X)\n",
    "\n",
    "print('Explained Variance')\n",
    "print('Our')\n",
    "print(pca.explained_variance)\n",
    "print()\n",
    "print('Sklearn')\n",
    "print(pca_skl.explained_variance_)\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Components')\n",
    "print('Our')\n",
    "print(pca.components)\n",
    "print()\n",
    "print('Sklearn')\n",
    "print(pca_skl.components_)\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Mean')\n",
    "print('Our')\n",
    "print(pca.mean)\n",
    "print()\n",
    "print('Sklearn')\n",
    "print(pca_skl.mean_)    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1/6.2 -> Implementing stratified splitting test using iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split\n",
      "Iris dataset: (150, 4)\n",
      "\n",
      "Training dataset: (120, 4)\n",
      "\n",
      "Testing dataset: (30, 4)\n",
      "\n",
      "\n",
      "Stratified train test split\n",
      "Iris dataset: (150, 4)\n",
      "\n",
      "Training dataset: (120, 4)\n",
      "\n",
      "Testing dataset: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split,stratified_train_test_split\n",
    "\n",
    "iris_dataset = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/iris/iris.csv', features=True, label=True)\n",
    "\n",
    "train, test = train_test_split(iris_dataset, test_size=0.2, random_state=42)    \n",
    "train_strat, test_strat = stratified_train_test_split(iris_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Train test split\")\n",
    "print(f\"Iris dataset: {iris_dataset.shape()}\")\n",
    "print()\n",
    "print(f\"Training dataset: {train.shape()}\")\n",
    "print()\n",
    "print(f\"Testing dataset: {test.shape()}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Stratified train test split\")\n",
    "print(f\"Iris dataset: {iris_dataset.shape()}\")\n",
    "print()\n",
    "print(f\"Training dataset: {train_strat.shape()}\")\n",
    "print()\n",
    "print(f\"Testing dataset: {test_strat.shape()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1 -> Implementing the RMSE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our rmse = 6.767569726275452\n",
      "sklearn rmse = 6.767569726275452\n"
     ]
    }
   ],
   "source": [
    "from si.metrics.rmse import rmse\n",
    "import numpy as np \n",
    "\n",
    "# compare with sklearn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mean_squared_error_skl\n",
    "from sklearn.metrics import mean_absolute_error as mean_absolute_error_skl\n",
    "from sklearn.metrics import r2_score as r2_score_skl\n",
    "\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1, 2, 3, 6, 20])\n",
    "\n",
    "\n",
    "our_rmse = rmse(y_true, y_pred)\n",
    "skl_rmse = mean_squared_error_skl(y_true, y_pred, squared=False)\n",
    "\n",
    "assert np.allclose(our_rmse, skl_rmse)\n",
    "\n",
    "print(f\"our rmse = {our_rmse}\")\n",
    "print(f\"sklearn rmse = {skl_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2/7.3 -> Implementing the KNNRegressor test using cpu.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of the model is: 81.3592188582519\n",
      "The rmse_skl of the model is: 81.36153382890387\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.models.knn_regressor import KNNRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from si.io.csv_file import read_csv\n",
    "\n",
    "dataset_ = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/cpu/cpu.csv', features=True, label=True)\n",
    "dataset_train, dataset_test = train_test_split(dataset_, test_size=0.2)\n",
    "\n",
    "knn = KNNRegressor(k=3)\n",
    "\n",
    "knn.fit(dataset_train)\n",
    "\n",
    "score = knn.score(dataset_test)\n",
    "print(f'The rmse of the model is: {score}')\n",
    "\n",
    "# compare with sklearn\n",
    "\n",
    "knn_skl = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "knn_skl.fit(dataset_train.X, dataset_train.y)\n",
    "\n",
    "score_skl = mean_squared_error(dataset_test.y, knn_skl.predict(dataset_test.X), squared=False)\n",
    "print(f'The rmse_skl of the model is: {score_skl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise -> Implementing the CategoricalNB Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Accuracy = 0.38666666666666666\n",
      "Sklearn Accuracy = 0.38\n"
     ]
    }
   ],
   "source": [
    "from si.models.categorical_nb import CategoricalNB\n",
    "from si.model_selection.split import train_test_split\n",
    "\n",
    "dataset = Dataset.from_random(1000, 5, 3)\n",
    "train, test = train_test_split(dataset, 0.3)\n",
    "nb = CategoricalNB()\n",
    "nb.fit(train)\n",
    "nb.predict(test)\n",
    "print(f\"Our Accuracy = {nb.score(test)}\")\n",
    "\n",
    "\n",
    "# test using sklearn \n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB as CategoricalNB_sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y, test_size=0.3, stratify=dataset.y)\n",
    "nb_sklearn = CategoricalNB_sklearn()\n",
    "nb_sklearn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Sklearn Accuracy = {nb_sklearn.score(X_test, y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 -> Implementing RidgeRegression with Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model\n",
      "Parameters: [0.58823529 1.08145743]\n",
      "Intercept: 8.5\n",
      "Score: 0.07698961937716277\n",
      "Predictions: [14.85294118]\n",
      "\n",
      "Compare with sklearn\n",
      "Parameters: [0.58823529 1.08145743]\n",
      "Intercept: 8.5\n",
      "Score: 0.07698961937716259\n",
      "Predictions: [15.67199303]\n"
     ]
    }
   ],
   "source": [
    "from si.data.dataset import Dataset\n",
    "from si.models.ridge_regression_least_squares import RidgeRegressionLeastSquares\n",
    "from si.metrics.mse import mse\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "dataset_ = Dataset(X=X, y=y)\n",
    "\n",
    "\n",
    "print('Our model')\n",
    "model = RidgeRegressionLeastSquares()\n",
    "model.fit(dataset_)\n",
    "\n",
    "print(f\"Parameters: {model.theta}\")\n",
    "print(f\"Intercept: {model.theta_zero}\")\n",
    "\n",
    "score = model.score(dataset_)\n",
    "print(f\"Score: {score}\")\n",
    "\n",
    "y_pred_ = model.predict(Dataset(X=np.array([[3, 5]])))\n",
    "print(f\"Predictions: {y_pred_}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print('Compare with sklearn')\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()    \n",
    "X = (dataset_.X - np.nanmean(dataset_.X, axis=0)) / np.nanstd(dataset_.X, axis=0)\n",
    "model.fit(X, dataset_.y)\n",
    "print(f\"Parameters: {model.coef_}\") # should be the same as theta\n",
    "print(f\"Intercept: {model.intercept_}\") # should be the same as theta_zero\n",
    "print(f\"Score: {mse(dataset_.y, model.predict(X))}\")\n",
    "print(f\"Predictions: {model.predict(np.array([[3, 5]]))}\") # should be the same as y_pred_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7\n",
    "\n",
    "## Exercise 9,1/9.2 -> Implementing the RandomForestClassifier class test using iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model\n",
      "Score 1.0\n",
      "\n",
      "Compare with sklearn\n",
      "Score 0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.models.random_forest_classifier import RandomForestClassifier\n",
    "\n",
    "data = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/iris/iris.csv', sep=',', features=True, label=True)\n",
    "train, test = train_test_split(data, test_size=0.33, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=5, min_samples_split=3, max_depth=3, mode='gini')\n",
    "model.fit(train)\n",
    "print('Our model')\n",
    "print(f'Score {model.score(test)}')\n",
    "print()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "print('Compare with sklearn')\n",
    "model = RFC(n_estimators=3, min_samples_split=3, max_depth=3, criterion='gini')\n",
    "model.fit(train.X, train.y)\n",
    "print(f'Score {model.score(test.X, test.y)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 -> Implementing the StackingClassifier ensemble test using breast-bin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa_bruno/anaconda3/envs/mestrado_si/lib/python3.11/site-packages/si-0.0.1-py3.11.egg/si/models/logistic_regression.py:176: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = (dataset.y * np.log(predictions)) + (1 - dataset.y) * np.log(1 - predictions)\n",
      "/home/sa_bruno/anaconda3/envs/mestrado_si/lib/python3.11/site-packages/si-0.0.1-py3.11.egg/si/models/logistic_regression.py:176: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = (dataset.y * np.log(predictions)) + (1 - dataset.y) * np.log(1 - predictions)\n",
      "/home/sa_bruno/anaconda3/envs/mestrado_si/lib/python3.11/site-packages/si-0.0.1-py3.11.egg/si/statistics/sigmoid_function.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model score: 0.9652173913043478\n",
      "Sklearn model score: 0.9695652173913043\n"
     ]
    }
   ],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "from si.ensemble.stacking_classifier import StackingClassifier\n",
    "\n",
    "data = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/breast_bin/breast-bin.csv', sep=',', features=True, label=True)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "knn = KNNClassifier(5)\n",
    "\n",
    "lr = LogisticRegression(0.01, 1000)\n",
    "\n",
    "dt = DecisionTreeClassifier(2, 10)\n",
    "\n",
    "knn2 = KNNClassifier(5)\n",
    "\n",
    "#create a StackingClassifier model using the previous classifiers. The second KNNClassifier model must be used as the final model.\n",
    "sc = StackingClassifier([knn, lr, dt], knn2)\n",
    "\n",
    "#Train the StackingClassifier model. What is the score of the model on the test set?\n",
    "sc.fit(train)\n",
    "print(f'Our model score: {sc.score(test)}')\n",
    "\n",
    "# sklearn \n",
    "from sklearn.ensemble import StackingClassifier as StackingClassifierSK\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#create a StackingClassifier model using the previous classifiers. The second KNNClassifier model must be used as the final model.\n",
    "sc = StackingClassifierSK(estimators=[('knn', KNeighborsClassifier(5)), ('lr', LogisticRegression()), ('dt', DecisionTreeClassifier())], final_estimator=KNeighborsClassifier(5))\n",
    "#Train the StackingClassifier model. What is the score of the model on the test set?\n",
    "sc.fit(train.X, train.y)\n",
    "print(f'Sklearn model score: {sc.score(test.X, test.y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 8\n",
    "## Exercise 11 -> Implementing the randomized_search_cv function test using breast-bin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: [6.0, 0.001, 1859.2964824120604]\n",
      "Best score: 0.9683908045977011\n"
     ]
    }
   ],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.model_selection.cross_validation import k_fold_cross_validation\n",
    "from si.model_selection.grid_search import grid_search_cv\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "breast_dataset = read_csv('/home/sa_bruno/Documentos/GitHub/si/datasets/breast_bin/breast-bin.csv', sep=',', features=True, label=True)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'l2_penalty': np.linspace(1, 10, 10),\n",
    "    'alpha': np.linspace(0.001, 0.0001, 100),\n",
    "    'max_iter': np.linspace(1000, 2000, 200)\n",
    "}\n",
    "\n",
    "scores = randomized_search_cv(lg,\n",
    "                        breast_dataset,\n",
    "                        hyperparameter_grid=hyperparameter_grid,\n",
    "                        cv=3,\n",
    "                        n_iter=10\n",
    "                        )\n",
    "\n",
    "print(f'Best hyperparameters: {scores[\"best_hyperparameters\"]}')\n",
    "print(f'Best score: {scores[\"best_score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9\n",
    "## Exercise 12.1 -> Implementing the Dropout layer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[6 3 7 4 6 9 2 6 7 4]]\n",
      "\n",
      "Output: [[12.  6.  0.  8. 12.  0.  0.  0.  0.  8.]]\n",
      "\n",
      "Backward pass: [[6 3 0 4 6 0 0 0 0 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.layers import Dropout\n",
    "\n",
    "dropout = Dropout(0.5)\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 10, size=(1, 10))\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {dropout.forward_propagation(X, training=True)}')\n",
    "print()\n",
    "\n",
    "print(f'Backward pass: {dropout.backward_propagation(X)}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13.1 -> Implementing the TanhActivation class test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[ 1 -2  2 -1  1  4 -3  1  2 -1]]\n",
      "\n",
      "Output: [[ 0.76159416 -0.96402758  0.96402758 -0.76159416  0.76159416  0.9993293\n",
      "  -0.99505475  0.76159416  0.96402758 -0.76159416]]\n",
      "\n",
      "Backward pass: [[ 0.41997434 -0.14130165  0.14130165 -0.41997434  0.41997434  0.0053638\n",
      "  -0.02959811  0.41997434  0.14130165 -0.41997434]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.activation import TanhActivation\n",
    "\n",
    "tanh = TanhActivation()\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(-5, 5, size=(1, 10))\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {tanh.forward_propagation(X, training=True)}')\n",
    "print()\n",
    "\n",
    "print(f'Backward pass: {tanh.backward_propagation(X)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13.2 -> Implementing the SoftmaxActivation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[3 4 2 4 4 1 2 2 2 4]]\n",
      "\n",
      "Output: [[0.07418408 0.20165325 0.0272908  0.20165325 0.20165325 0.01003972\n",
      "  0.0272908  0.0272908  0.0272908  0.20165325]]\n",
      "\n",
      "Backward pass: [[0.20604242 0.64395686 0.05309202 0.64395686 0.64395686 0.00993893\n",
      "  0.05309202 0.05309202 0.05309202 0.64395686]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.activation import SoftmaxActivation\n",
    "\n",
    "softmax = SoftmaxActivation()\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 5, size=(1, 10))\n",
    "\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {softmax.forward_propagation(X, training=True)}')\n",
    "print()\n",
    "\n",
    "print(f'Backward pass: {softmax.backward_propagation(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14 -> Implementing the CategoricalCrossEntropy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[3 4 2 4 4]]\n",
      "\n",
      "Target: [0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      "\n",
      "Output: 13.255676952536477\n",
      "\n",
      "Gradient: [[ -8.00982284  -4.20736279  -2.73226305  -6.6816058  -25.63796216]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.losses import CategoricalCrossEntropy\n",
    "\n",
    "\n",
    "cce = CategoricalCrossEntropy()\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 5, size=(1, 5))\n",
    "\n",
    "np.random.seed(42)\n",
    "y = np.random.random(5)\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "print(f'Target: {y}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {cce.loss(X, y)}')\n",
    "print()\n",
    "print(f'Gradient: {cce.derivative(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 15 -> Implementing the Adam class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[3 4 2 4 4]]\n",
      "\n",
      "Target: [0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
      "\n",
      "Output: [[2.99 3.99 1.99 3.99 3.99]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.optimizers import Adam\n",
    "\n",
    "adam = Adam()\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 5, size=(1, 5))\n",
    "\n",
    "np.random.seed(42)\n",
    "y = np.random.random(5)\n",
    "\n",
    "print(f'Input: {X}')\n",
    "print()\n",
    "print(f'Target: {y}')\n",
    "print()\n",
    "\n",
    "print(f'Output: {adam.update(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 16 -> Build, train and evaluate a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 01:29:44.846866: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 01:29:44.912865: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 01:29:45.251576: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 01:29:45.251702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 01:29:45.307289: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 01:29:45.427655: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 01:29:45.430021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 01:29:47.968669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Test score: 0.54\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.0000e+00\n",
      "Keras Test score: [0.6855418682098389, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.losses import BinaryCrossEntropy\n",
    "from si.neural_networks.optimizers import SGD\n",
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "from si.data.dataset import Dataset\n",
    "from si.model_selection.split import train_test_split\n",
    "from si.data.dataset import Dataset\n",
    "from si.neural_networks.layers import DenseLayer\n",
    "from si.neural_networks.activation import ReLUActivation, SigmoidActivation\n",
    "from si.neural_networks.losses import BinaryCrossEntropy\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.model_selection.split import train_test_split\n",
    "\n",
    "dataset = Dataset(X=np.random.rand(500, 32), y=np.random.randint(0, 2, 500))\n",
    "train, test = train_test_split(dataset)\n",
    "\n",
    "net = NeuralNetwork(epochs=100, batch_size=16, optimizer=SGD, learning_rate=0.01, verbose=False,\n",
    "                    loss=BinaryCrossEntropy, metric=accuracy)\n",
    "n_features = dataset.X.shape[1]\n",
    "\n",
    "net.add(DenseLayer(16, (n_features,)))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DenseLayer(8))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DenseLayer(1))\n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "\n",
    "net.fit(train)\n",
    "\n",
    "# compare with keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "dataset = Dataset(X=np.random.rand(500, 32), y=np.random.randint(0, 2, 500))\n",
    "train, test = train_test_split(dataset)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(32,)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss=BinaryCrossentropy(), metrics=[Accuracy()])\n",
    "model.fit(train.X, train.y, epochs=100, batch_size=16, verbose=False)\n",
    "\n",
    "print(f'Our Test score: {net.score(test)}')\n",
    "print(f'Keras Test score: {model.evaluate(test.X, test.y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 11\n",
    "## Exercise 17 -> Implementing the One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['b', 'r', 'u', 'n', 'o', 's', 'a']\n",
      "Output: [[[0, 0, 1, 0, 0, 0, 0]], [[0, 0, 0, 0, 1, 0, 0]], [[1, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 1, 0, 0, 0]], [[0, 1, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 1]], [[0, 0, 0, 0, 0, 1, 0]]]\n",
      "Inverse transform: [['b'], ['r'], ['u'], ['n'], ['o'], ['s'], ['a']]\n",
      "\n",
      "With padder\n",
      "Input: ['b', 'r', 'u', 'n', 'o', 's', 'a']\n",
      "Output: [[[0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], [[0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0]], [[1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], [[0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], [[0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0]], [[0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0]], [[0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0]]]\n",
      "Inverse transform: [['b', 'a'], ['r', 'a'], ['u', 'a'], ['n', 'a'], ['o', 'a'], ['s', 'a'], ['a', 'a']]\n"
     ]
    }
   ],
   "source": [
    "from si.encoding.one_hot_encoder import OneHotEncoder\n",
    "\n",
    "data = ['b', 'r', 'u', 'n', 'o', 's', 'a']\n",
    "\n",
    "# create a one hot encoder\n",
    "encoder = OneHotEncoder(padder=None, max_length=None)\n",
    "\n",
    "# fit the encoder\n",
    "encoder.fit(data)\n",
    "\n",
    "# transform the data\n",
    "print(f'Input: {data}')\n",
    "\n",
    "# transform the data\n",
    "print(f'Output: {encoder.transform(data)}')\n",
    "\n",
    "# inverse transform the data\n",
    "print(f'Inverse transform: {encoder.inverse_transform(encoder.transform(data))}')\n",
    "\n",
    "print()\n",
    "print(\"With padder\")\n",
    "\n",
    "encoder_p = OneHotEncoder(padder='a', max_length=2)\n",
    "\n",
    "# fit the encoder\n",
    "encoder_p.fit(data)\n",
    "\n",
    "# transform the data\n",
    "print(f'Input: {data}')\n",
    "\n",
    "# transform the data\n",
    "print(f'Output: {encoder_p.transform(data)}')\n",
    "\n",
    "# inverse transform the data\n",
    "print(f'Inverse transform: {encoder_p.inverse_transform(encoder_p.transform(data))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 18 -> Implementing the CNN\n",
    "\n",
    "PATH: /si/scripts/cnn_exercise.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mestrado_si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
